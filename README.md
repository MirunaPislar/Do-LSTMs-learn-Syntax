# Do LSTMs learn Syntax?

## Some relevant papers:

[Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies](http://aclweb.org/anthology/Q16-1037) Linzen et al. (2016) 

[Recurrent Neural Network Grammars](https://www.aclweb.org/anthology/N16-1024) Dyer et al. (2016)

[Exploring the Syntactic Abilities of RNNs with Multi-task Learning](http://aclweb.org/anthology/K17-1003) Enguehard et al. (2017)

[What Do Recurrent Neural Network Grammars Learn About Syntax?](http://aclweb.org/anthology/E17-1117) Kuncoro et al. (2017)

[Colorless green recurrent networks dream hierarchically](http://aclweb.org/anthology/N18-1108) Gulordava et al. (2018)

[The Importance of Being Recurrent for Modeling Hierarchical Structure](http://aclweb.org/anthology/D18-1503) Tran et al. (2018)

[Targeted Syntactic Evaluation of Language Models](http://aclweb.org/anthology/D18-1151) Marvin and Linzen (2018)

[Do RNNs learn human-like abstract word order preferences?](https://arxiv.org/pdf/1811.01866.pdf) Futrell and Levy (2018)

[What do RNN Language Models Learn about Filler–Gap Dependencies?](http://aclweb.org/anthology/W18-5423) Wilcox et al. (2018)

[LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better](http://aclweb.org/anthology/P18-1132) Kuncoro et al. (2018)

[Finding Syntax in Human Encephalography with Beam Search](http://aclweb.org/anthology/P18-1254) Hale et al. (2018)

[Assessing BERT’s Syntactic Abilities](https://arxiv.org/pdf/1901.05287.pdf) Goldberg (2019)
