# Do LSTMs learn Syntax?

## Some relevant papers:

[Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies](http://aclweb.org/anthology/Q16-1037), Linzen et al. (2016) [[code]](https://github.com/TalLinzen/rnn_agreement)

[Recurrent Neural Network Grammars](https://www.aclweb.org/anthology/N16-1024), Dyer et al. (2016)

[Exploring the Syntactic Abilities of RNNs with Multi-task Learning](http://aclweb.org/anthology/K17-1003), Enguehard et al. (2017)

[What Do Recurrent Neural Network Grammars Learn About Syntax?](http://aclweb.org/anthology/E17-1117), Kuncoro et al. (2017)

[On the State of the Art of Evaluation in Neural Language Models](https://arxiv.org/pdf/1707.05589.pdf), Melis et al. (2017) 

[Colorless green recurrent networks dream hierarchically](http://aclweb.org/anthology/N18-1108), Gulordava et al. (2018) [[code]](https://github.com/facebookresearch/colorlessgreenRNNs)

[The Importance of Being Recurrent for Modeling Hierarchical Structure](http://aclweb.org/anthology/D18-1503), Tran et al. (2018)

[Targeted Syntactic Evaluation of Language Models](http://aclweb.org/anthology/D18-1151), Marvin and Linzen (2018) [[code]](https://github.com/BeckyMarvin/LM_syneval)

[What can linguistics and deep learning contribute to each other?](https://arxiv.org/pdf/1809.04179.pdf), Linzen (2018)

[Do RNNs learn human-like abstract word order preferences?](https://arxiv.org/pdf/1811.01866.pdf), Futrell and Levy (2018) [[code]](https://github.com/langprocgroup/rnn_soft_constraints)

[What do RNN Language Models Learn about Filler–Gap Dependencies?](http://aclweb.org/anthology/W18-5423), Wilcox et al. (2018)

[LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better](http://aclweb.org/anthology/P18-1132), Kuncoro et al. (2018)

[On Evaluating the Generalization of LSTM Models in Formal Languages](https://arxiv.org/pdf/1811.01001.pdf), Suzgun et al. [[code]](https://github.com/suzgunmirac/lstm-eval)

[Evaluating the Ability of LSTMs to Learn Context-Free Grammars](http://aclweb.org/anthology/W18-5414), Sennhauser and Berwick (2018)

[Finding Syntax in Human Encephalography with Beam Search](http://aclweb.org/anthology/P18-1254), Hale et al. (2018)

[Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context](http://aclweb.org/anthology/P18-1027), Khandelwal et al. (2018)

[Assessing BERT’s Syntactic Abilities](https://arxiv.org/pdf/1901.05287.pdf), Goldberg (2019) [[code]](https://github.com/yoavg/bert-syntax)
